---
title: "How to do a SLR analysis with R"
output: 
  html_document: 
    theme: paper
    toc: yes
    toc_float: yes
---

We are going to use the dataset *cars* available in R.    

```{r data}

# ?cars
summary(cars)

```

# Point estimates

R will calculate the linear regression line with the *lm* function.   
We will store the results in an object called *cars.lm*.

```{r regression}

cars.lm <- lm(dist ~ speed, data = cars)
names(cars.lm)
coef(cars.lm)

```

We visually inspect the data:

```{r regressionplot}

plot(dist ~ speed, data = cars, pch = 16)
abline(coef(cars.lm), col="red", lwd=2)
summary(cars.lm)

```


## Exercises

1) What is the meaning of 60 = b0+b1(8)?   

2) Interpret the slope

3) Interpret the intercept

4) Use the regression line to obtain a point estimate of the mean stopping distance for a car travelling 8 mph.

```{r pointestimate}

-17.58 + 3.93 * (8) # point estimate
cars[5,] # real data

```

# Fitted and predicted values

The estimates of *Y* for a value of *x* that matches an observed value is called a *fitted value*.    
To get the fitted values in R:

```{r fittedvalues}

fitted(cars.lm)
plot(dist ~ speed, data = cars, pch = 16)
abline(coef(cars.lm), col="red")
points(cars$speed, fitted(cars.lm), col="red", pch=19)

```

Prediction at *x* values that are not necessarily part of the original data are done with the *predict* function.

```{r predictedvalues}

predict(cars.lm, newdata = data.frame(speed = c(6,8,21)))
plot(dist ~ speed, data = cars, pch = 16)
abline(coef(cars.lm), col="red")
points(cars$speed, fitted(cars.lm), col="red", pch=19)
points(c(6,8,21),
       predict(cars.lm, newdata = data.frame(speed = c(6,8,21))),
       col="blue", pch=19)
```


# Residuals

The residuals for the model may be obtained with the *residuals* function.

```{r residuals}

residuals(cars.lm)
# Residual standard error
carsumry <- summary(cars.lm)
carsumry$sigma

```


# An overall summary for the SLR

```{r lmsummary}

summary(cars.lm)

```

In the *Coefficients* section we find the parameter estimates and their respective standard errors in the second and third columns. But we can also obtain the 95% confidence intervals:

```{r coefconfidenceintervals}

confint(cars.lm)

```

These are the confidence intervals for the parameters, but, in general, how good are our *Y* estimates? How much confidence do we have in these estimates?

```{r confidencebands}

# new <- data.frame(speed = c(5,6,21))
# predict(cars.lm, newdata = new, interval = "confidence")
# predict(cars.lm, newdata = new, interval = "prediction")
library(HH)
ci.plot(cars.lm)
# or
# library(UsingR)
# simple.lm(cars$speed, cars$dist, show.ci=TRUE)

```

Note that we have two types of confidence intervals. The confidence is for the mean, and the prediction for the individual.

## Types of confidence intervals

The following are the types of confidence intervals used for predictions in regression and other linear models:

* Prediction interval - Provides a range of likely values for a single response.
* Confidence interval of the prediction - Provides a range of likely values for the mean response.

For example, here we developed a regression model relating the stopping distance of a car to the car speed. You feel confident that the model accurately fits the data. Therefore, you conclude that it is acceptable to use the model to predict the stopping distance.

For each prediction, you specify the value for the predictor and set the confidence level at 95%. The result is the widest 95% prediction interval. You can be 95% confident that this range includes the value of the new observation.
**A prediction interval is a range that is likely to contain the response value of a single new observation given specified settings of the predictors in your model.**

Furthermore, the 95% confidence interval of the prediction is more narrow. You can be 95% confident that this range includes the mean response for all cars travelling that speed.
**A confidence interval of the prediction is a range that is likely to contain the mean response given specified settings of the predictors in your model. Just like the regular confidence intervals, the confidence interval of the prediction presents a range for the mean rather than the distribution of individual data points.**

The prediction interval is always wider than the confidence interval because of the added uncertainty involved in predicting a single response versus the mean response.

(see http://blog.minitab.com/blog/adventures-in-statistics-2/when-should-i-use-confidence-intervals-prediction-intervals-and-tolerance-intervals)

# R2

There are several definitions of R2 that are only sometimes equivalent. When an intercept is included, then it is simply the square of the sample correlation coefficient (i.e., r) between the observed outcomes and the observed predictor values. If additional regressors are included, R2 is the square of the coefficient of multiple correlation. In both such cases, the coefficient of determination ranges from 0 to 1.

The use of an adjusted R2 is an attempt to take account of the phenomenon of the R2 automatically and spuriously increasing when extra explanatory variables are added to the model. It is a modification of R2 that adjusts for the number of explanatory terms in a model relative to the number of data points. The adjusted R2 can be negative, and its value will always be less than or equal to that of R2. Unlike R2, the adjusted R2 increases only when the increase in R2 (due to the inclusion of a new explanatory variable) is more than one would expect to see by chance.

# The degrees of freedom

* Corrected Degrees of Freedom for Model: parameters - 1
* Degrees of Freedom for Error: number of data - parameters

# The heart rate example

The maximum heart rate of a person if often said to be related to age by the equation: Max = 220 - 1*Age. The expected value for a regression coefficient of age hearth rate (max) and age is 1.
Let's check if the data support this statement. (Try to formulate the null and alternative hypothesis first!)

```{r heartratedata}

x = c(18, 23, 25, 35, 65, 54, 34, 56, 72, 19, 23, 42, 18, 39, 37)
y = c(202, 186, 187, 180, 156, 169, 174, 172, 153, 199, 193, 174, 198, 183, 178)
lm(y ~ x) # the basic values of the regression analysis
lm.result = lm(y ~ x)
summary(lm.result)
plot(x,y, xlim=c(0,80), ylim=c(150,240)) # make a plot
abline(coef(lm.result)) #lm(y ~ x)) # plot the regression line

# to see only some parts of the result
coef(lm.result)
summary(resid(lm.result))

```

We can do a test to see if the slope of -1 is correct. In this case the null hypothesis is that the slope is -1, vs. the alternative hypothesis that the slope is different from -1. We can create the test statistic and find the p-value by hand:

```{r heartrate-handtest}
# COMMENTED LINES - CALCULATIONS BY HAND, YOU CAN SKIP THEM AND USE THE OUTPUT OF THE REGRESSION ANALYSIS
# n = length(x)
# es = resid(lm.result)
# b1 = coef(lm.result)[[2]]
# s = sqrt(sum(es^2)/(n-2)) # S^2 = SSE/(n-2), s is the residual standard error
# SE = s/sqrt(sum((x-mean(x))^2)) # SE
b1 = -0.79773
SE = 0.06996
t = (b1-(-1))/SE # (obs. value - hyp.value)/SE
t
pt(t, 13, lower.tail = FALSE) # FIND THE RIGHT TAIL FOR THIS VALUE OF t AND 15-2 df
# multiply by 2 for a two-sided test
pt(t, 13, lower.tail = FALSE)*2 # < 0.05, REJECT H0: it is unlikely that for this data 

```

Now let's try to do something similar for the intercept. Let's check if the data support the intercept of 220 (H0: b0=220; H1: b0 < 220)

```{r heartrate-handtest-intercept}
# COMMENTED LINES - CALCULATIONS BY HAND, YOU CAN SKIP THEM AND USE THE OUTPUT OF THE REGRESSION ANALYSIS
# SE.b0 = s*(sqrt((1/n)+(mean(x)^2/sum((x-mean(x))^2)))) # formula for the standard error of the intercept
SE.b0 = 2.86694
b0 = coef(lm.result)[[1]]
t = (b0 - 220)/SE.b0
t
pt(t, 13, lower.tail = TRUE) # FIND THE LEFT TAIL FOR THIS VALUE OF t AND 15-2 df

```

